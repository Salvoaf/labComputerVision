{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Salvoaf/labComputerVision/blob/main/2_Document_term_matrix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMkY2PkdfFTD"
      },
      "source": [
        "# Document-term matrix generation\n",
        "\n",
        "In this exercise, you'll have to generate a document-term matrix from an input list of preprocessed documents."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuLmjEz-fPHE",
        "outputId": "e1df8a54-f222-49f0-da6f-7a8af9932112"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipytest\n",
            "  Downloading ipytest-0.12.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipytest) (7.9.0)\n",
            "Collecting pytest>=5.4\n",
            "  Downloading pytest-7.1.3-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ipytest) (21.3)\n",
            "Collecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (5.0.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (22.1.0)\n",
            "Collecting iniconfig\n",
            "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.11.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (4.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.0.10)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (5.1.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.6.1)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 40.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->ipytest) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->ipytest) (3.0.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipytest) (0.7.0)\n",
            "Installing collected packages: pluggy, jedi, iniconfig, pytest, ipytest\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed iniconfig-1.1.1 ipytest-0.12.0 jedi-0.18.1 pluggy-1.0.0 pytest-7.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Q6a-E8oVfFTH"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "import ipytest\n",
        "import pytest\n",
        "\n",
        "ipytest.autoconfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JM6G7JYfFTJ"
      },
      "source": [
        "Input documents are given as lists of tokenized terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nWE2GbD7fFTJ"
      },
      "outputs": [],
      "source": [
        "DOCUMENTS = [\n",
        "    [\"aaa\", \"bbb\", \"ccc\"],\n",
        "    [\"eee\", \"fff\"],\n",
        "    [\"aaa\", \"eee\", \"aaa\", \"ccc\", \"fff\", \"fff\", \"ggg\", \"aaa\"],\n",
        "    [\"bbb\", \"bbb\", \"bbb\"],\n",
        "    [\"ggg\", \"fff\", \"ccc\", \"aaa\", \"ccc\"],\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Jo8RFoJ9DcYy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlseFHTXfFTJ"
      },
      "source": [
        "Your task is to complete this method:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Soluzione 1"
      ],
      "metadata": {
        "id": "Zce-_FtaDaaT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "bgfotiDufFTK"
      },
      "outputs": [],
      "source": [
        "def get_doc_term_matrix(docs: List[List[str]]) -> Tuple[List[List[int]], List[str]]:\n",
        "    \"\"\"Generates a document-term matrix and the corresponding vocabulary.\n",
        "    \n",
        "    Args:\n",
        "        docs: List of documents, each given by a list of tokenized terms.\n",
        "        \n",
        "    Returns:\n",
        "        Tuple consisting of the document-term matrix and the corresponding vocabulary.\n",
        "        In the document-term matrix row `i` corresponds to `docs[i]` and column `j`\n",
        "        corresponds to the jth element of the vocabulary. Values represent the number\n",
        "        of times the term appears in the document.\n",
        "        Terms may be in any order in the vocabulary.\n",
        "    \"\"\"\n",
        "    vocabulary = []\n",
        "    doc_term_matrix = []\n",
        "    vector = []\n",
        "    for doc in docs:\n",
        "      for word in doc:\n",
        "        if word not in vocabulary:\n",
        "          vocabulary.append(word)\n",
        "    for doc in docs:\n",
        "      vector = [0 for i in range(0, len(vocabulary))]\n",
        "      for word in doc:\n",
        "        for i,voc_word in enumerate(vocabulary):\n",
        "          if word == voc_word :\n",
        "            vector[i] = vector[i]+1\n",
        "            continue;\n",
        "      doc_term_matrix.append(vector)\n",
        "    return doc_term_matrix, vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Soluzione 2"
      ],
      "metadata": {
        "id": "iXZ7UsWsDdgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_doc_term_matrix(docs: List[List[str]]) -> Tuple[List[List[int]], List[str]]:\n",
        "    \"\"\"Generates a document-term matrix and the corresponding vocabulary.\n",
        "    \n",
        "    Args:\n",
        "        docs: List of documents, each given by a list of tokenized terms.\n",
        "        \n",
        "    Returns:\n",
        "        Tuple consisting of the document-term matrix and the corresponding vocabulary.\n",
        "        In the document-term matrix row `i` corresponds to `docs[i]` and column `j`\n",
        "        corresponds to the jth element of the vocabulary. Values represent the number\n",
        "        of times the term appears in the document.\n",
        "        Terms may be in any order in the vocabulary.\n",
        "    \"\"\"\n",
        "    vocabulary = [] #totale parole presenti in tutti i documenti\n",
        "    doc_term_matrix = []\n",
        "    vector = [] #una lista di dizionari che memorizzano per ogni parola del documento(key) la proprio occorrenza(value)\n",
        "    dictionary = {} #per ogni parola del documento(key) la proprio occorrenza(value)\n",
        "\n",
        "    for doc in docs: #estraggo ogni documento\n",
        "      dictionary = {}\n",
        "      for word in doc:#estraggo ogni parola del documento in questione\n",
        "        if word in dictionary.keys(): #controllo se nel dizionario è presente una certa parola del doc\n",
        "          dictionary[word] = dictionary[word] +1 #se ho già la parola aumento l'occorrenza\n",
        "        else:\n",
        "          dictionary[word] =  1 #se è la prima volta che la parola si presenta nel dizionario\n",
        "        if word not in vocabulary: \n",
        "          vocabulary.append(word)\n",
        "      vector.append(dictionary)\n",
        "\n",
        "\n",
        "    for doc in vector:\n",
        "      list_vec = [] #utilizzo questa lista per popolare il nostro indice\n",
        "      for word in vocabulary:\n",
        "        if word in doc.keys():\n",
        "          list_vec.append(doc[word])\n",
        "        else:\n",
        "          list_vec.append(0)\n",
        "      doc_term_matrix.append(list_vec)\n",
        "    return doc_term_matrix, vocabulary\n",
        "      \n"
      ],
      "metadata": {
        "id": "UREDZBMjDkk-"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOl6WQCqfFTK"
      },
      "source": [
        "Tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufxiVksFfFTL",
        "outputId": "e95c2c44-78b7-4c37-c050-467e7416ae86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                          [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%%ipytest\n",
        "\n",
        "def test_num_docs():\n",
        "    doc_term_matrix, _ = get_doc_term_matrix(DOCUMENTS)\n",
        "    assert len(doc_term_matrix) == len(DOCUMENTS)\n",
        "    \n",
        "def test_vocabulary():\n",
        "    _, vocabulary = get_doc_term_matrix(DOCUMENTS)\n",
        "    assert set(vocabulary) == {\"aaa\", \"bbb\", \"ccc\", \"eee\", \"fff\", \"ggg\"}\n",
        "    \n",
        "def test_term_counts():\n",
        "    doc_term_matrix, vocabulary = get_doc_term_matrix(DOCUMENTS)\n",
        "    idx_aaa = vocabulary.index(\"aaa\")\n",
        "    idx_ccc = vocabulary.index(\"ccc\")\n",
        "    idx_fff = vocabulary.index(\"fff\")\n",
        "    assert doc_term_matrix[0][idx_aaa] == 1\n",
        "    assert doc_term_matrix[0][idx_ccc] == 1\n",
        "    assert doc_term_matrix[0][idx_fff] == 0\n",
        "    assert doc_term_matrix[2][idx_aaa] == 3\n",
        "    assert doc_term_matrix[2][idx_ccc] == 1\n",
        "    assert doc_term_matrix[2][idx_fff] == 2"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "d6a0b9ba27f634b55723b9a72ccf6e1561be2239a81593bce53747f2fee7a1a2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}