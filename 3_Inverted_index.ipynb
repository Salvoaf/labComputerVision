{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Salvoaf/labComputerVision/blob/main/3_Inverted_index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARlqK1n6b3P9"
      },
      "source": [
        "# Building an inverted index\n",
        "\n",
        "  - You are given a sample (1000 documents) from the [The Reuters-21578 data collection](http://www.daviddlewis.com/resources/testcollections/reuters21578/) in `data/reuters21578-000.xml`\n",
        "  - The code that parses the XML and extract a list of preprocessed terms (tokenized, lowercased, stopwords removed) is already given\n",
        "  - You are also given an `InvertedIndex` class that manages the posting lists operations\n",
        "  - Your task is to build an inverted index from the input collection."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Salvoaf/labComputerVision.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSM-9e_Aryx2",
        "outputId": "7dbba361-0b08-4820-d01f-060e04fb4f59"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'labComputerVision'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 21 (delta 8), reused 6 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (21/21), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV81UecqjQlM",
        "outputId": "c9ab7a7b-6555-41d4-9051-17ca834cc580"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipytest\n",
            "  Downloading ipytest-0.12.0-py3-none-any.whl (15 kB)\n",
            "Collecting pytest>=5.4\n",
            "  Downloading pytest-7.1.3-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipytest) (7.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ipytest) (21.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (2.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (5.0.0)\n",
            "Collecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting iniconfig\n",
            "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (22.1.0)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.11.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (4.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.0.10)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (5.1.1)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 44.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (57.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->ipytest) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->ipytest) (3.0.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipytest) (0.7.0)\n",
            "Installing collected packages: pluggy, jedi, iniconfig, pytest, ipytest\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed iniconfig-1.1.1 ipytest-0.12.0 jedi-0.18.1 pluggy-1.0.0 pytest-7.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HtF5ZST_b3QB"
      },
      "outputs": [],
      "source": [
        "import ipytest\n",
        "import re\n",
        "\n",
        "from typing import List, Dict, Union, Any, Callable\n",
        "from collections import Counter, defaultdict\n",
        "from xml.dom import minidom\n",
        "from dataclasses import dataclass\n",
        "\n",
        "ipytest.autoconfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAO9mdGMb3QD"
      },
      "source": [
        "## Parsing documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRTT0VOcb3QE"
      },
      "source": [
        "Stopwords list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6EyYzTdPb3QF"
      },
      "outputs": [],
      "source": [
        "STOPWORDS = [\"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\", \"in\", \"into\", \"is\", \"it\", \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\", \"the\", \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"to\", \"was\", \"will\", \"with\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsu3FWMKb3QF"
      },
      "source": [
        "Stripping tags inside <> using regex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M9Gq8vZob3QG"
      },
      "outputs": [],
      "source": [
        "def striptags(text: str) -> str:\n",
        "    \"\"\"Removes xml tags.\n",
        "\n",
        "    Args:\n",
        "        text: Text string with xml tags.\n",
        "\n",
        "    Returns:\n",
        "        String without xml tags.\n",
        "    \"\"\"\n",
        "    p = re.compile(r\"<.*?>\")\n",
        "    return p.sub(\"\", text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0z943Lab3QG"
      },
      "source": [
        "Parse input text and return a list of indexable terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Amdfmhs5b3QH"
      },
      "outputs": [],
      "source": [
        "def parse(text: str) -> List[str]:\n",
        "    \"\"\"Parses documents and removes xml tags and punctuation.\n",
        "\n",
        "    Args:\n",
        "        text: Text to parse.\n",
        "\n",
        "    Returns:\n",
        "        List of tokens.\n",
        "    \"\"\"\n",
        "    terms = []\n",
        "    # Replace specific characters with space\n",
        "    chars = [\"'\", \".\", \":\", \",\", \"!\", \"?\", \"(\", \")\"]\n",
        "    for ch in chars:\n",
        "        text = text.replace(ch, \" \")\n",
        "\n",
        "    # Remove tags\n",
        "    text = striptags(text)\n",
        "\n",
        "    # Tokenization\n",
        "    # default behavior of the split is to split on one or more whitespaces\n",
        "    return [term.lower() for term in text.split() if term not in STOPWORDS]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMu6suwbb3QI"
      },
      "source": [
        "## Processing the input document collection\n",
        "\n",
        "  - The collection is given as a single XML file. \n",
        "  - Each document is inside `<REUTERS ...> </REUTERS>`.\n",
        "  - We extract the contents of the `<DATE>`, `<TITLE>`, and `<BODY>` tags.\n",
        "  - After each extracted document, the provided callback function is called and all document data is passed in a single dict argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zk4ti8A_b3QJ"
      },
      "outputs": [],
      "source": [
        "def process_collection(input_file:str, callback: Callable) -> None:\n",
        "    \"\"\"Processes file and calls the callback function for each document in the\n",
        "    file.\n",
        "\n",
        "    Args:\n",
        "        input_file: Path to file to process.\n",
        "        callback: Function that will be called for each document.\n",
        "    \"\"\"\n",
        "    xmldoc = minidom.parse(input_file)\n",
        "    # Iterate documents in the XML file\n",
        "    itemlist = xmldoc.getElementsByTagName(\"REUTERS\")\n",
        "    for doc_id, doc in enumerate(itemlist):\n",
        "        date = doc.getElementsByTagName(\"DATE\")[0].firstChild.nodeValue\n",
        "        # Skip documents without a title or body\n",
        "        if not (doc.getElementsByTagName(\"TITLE\") and doc.getElementsByTagName(\"BODY\")):\n",
        "            continue\n",
        "        title = doc.getElementsByTagName(\"TITLE\")[0].firstChild.nodeValue\n",
        "        body = doc.getElementsByTagName(\"BODY\")[0].firstChild.nodeValue\n",
        "        callback({\n",
        "            \"doc_id\": doc_id+1,\n",
        "            \"date\": date,\n",
        "            \"title\": title,\n",
        "            \"body\": body\n",
        "            })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceYl1uX8b3QJ"
      },
      "source": [
        "Prints a document\"s contents (used as a callback function passed to `process_collection`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "e8M_l6STb3QJ"
      },
      "outputs": [],
      "source": [
        "def print_doc(doc: Dict[str, Union[str, int]]) -> None:\n",
        "    \"\"\"Print details of the first 5 documents.\n",
        "\n",
        "    Args:\n",
        "        doc: Dictionary with document details.\n",
        "    \"\"\"\n",
        "    if doc[\"doc_id\"] <= 5:  # print only the first 5 documents\n",
        "        print(\"docID:\", doc[\"doc_id\"])\n",
        "        print(\"date:\", doc[\"date\"])\n",
        "        print(\"title:\", doc[\"title\"])\n",
        "        print(\"body:\", doc[\"body\"])\n",
        "        print(\"--\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1btO0OIqP2E",
        "outputId": "5c778481-fa32-4137-9d54-8dfda888bcdb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "R14DHrHfb3QK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b240241b-6c1a-42e0-f1ca-3f5341981e55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "docID: 1\n",
            "date: 26-FEB-1987 15:01:01.79\n",
            "title: BAHIA COCOA REVIEW\n",
            "body: Showers continued throughout the week in\n",
            "the Bahia cocoa zone, alleviating the drought since early\n",
            "January and improving prospects for the coming temporao,\n",
            "although normal humidity levels have not been restored,\n",
            "Comissaria Smith said in its weekly review.\n",
            "    The dry period means the temporao will be late this year.\n",
            "    Arrivals for the week ended February 22 were 155,221 bags\n",
            "of 60 kilos making a cumulative total for the season of 5.93\n",
            "mln against 5.81 at the same stage last year. Again it seems\n",
            "that cocoa delivered earlier on consignment was included in the\n",
            "arrivals figures.\n",
            "    Comissaria Smith said there is still some doubt as to how\n",
            "much old crop cocoa is still available as harvesting has\n",
            "practically come to an end. With total Bahia crop estimates\n",
            "around 6.4 mln bags and sales standing at almost 6.2 mln there\n",
            "are a few hundred thousand bags still in the hands of farmers,\n",
            "middlemen, exporters and processors.\n",
            "    There are doubts as to how much of this cocoa would be fit\n",
            "for export as shippers are now experiencing dificulties in\n",
            "obtaining +Bahia superior+ certificates.\n",
            "    In view of the lower quality over recent weeks farmers have\n",
            "sold a good part of their cocoa held on consignment.\n",
            "    Comissaria Smith said spot bean prices rose to 340 to 350\n",
            "cruzados per arroba of 15 kilos.\n",
            "    Bean shippers were reluctant to offer nearby shipment and\n",
            "only limited sales were booked for March shipment at 1,750 to\n",
            "1,780 dlrs per tonne to ports to be named.\n",
            "    New crop sales were also light and all to open ports with\n",
            "June/July going at 1,850 and 1,880 dlrs and at 35 and 45 dlrs\n",
            "under New York july, Aug/Sept at 1,870, 1,875 and 1,880 dlrs\n",
            "per tonne FOB.\n",
            "    Routine sales of butter were made. March/April sold at\n",
            "4,340, 4,345 and 4,350 dlrs.\n",
            "    April/May butter went at 2.27 times New York May, June/July\n",
            "at 4,400 and 4,415 dlrs, Aug/Sept at 4,351 to 4,450 dlrs and at\n",
            "2.27 and 2.28 times New York Sept and Oct/Dec at 4,480 dlrs and\n",
            "2.27 times New York Dec, Comissaria Smith said.\n",
            "    Destinations were the U.S., Covertible currency areas,\n",
            "Uruguay and open ports.\n",
            "    Cake sales were registered at 785 to 995 dlrs for\n",
            "March/April, 785 dlrs for May, 753 dlrs for Aug and 0.39 times\n",
            "New York Dec for Oct/Dec.\n",
            "    Buyers were the U.S., Argentina, Uruguay and convertible\n",
            "currency areas.\n",
            "    Liquor sales were limited with March/April selling at 2,325\n",
            "and 2,380 dlrs, June/July at 2,375 dlrs and at 1.25 times New\n",
            "York July, Aug/Sept at 2,400 dlrs and at 1.25 times New York\n",
            "Sept and Oct/Dec at 1.25 times New York Dec, Comissaria Smith\n",
            "said.\n",
            "    Total Bahia sales are currently estimated at 6.13 mln bags\n",
            "against the 1986/87 crop and 1.06 mln bags against the 1987/88\n",
            "crop.\n",
            "    Final figures for the period to February 28 are expected to\n",
            "be published by the Brazilian Cocoa Trade Commission after\n",
            "carnival which ends midday on February 27.\n",
            " Reuter\n",
            "\n",
            "--\n",
            "docID: 2\n",
            "date: 26-FEB-1987 15:02:20.00\n",
            "title: STANDARD OIL <SRD> TO FORM FINANCIAL UNIT\n",
            "body: Standard Oil Co and BP North America\n",
            "Inc said they plan to form a venture to manage the money market\n",
            "borrowing and investment activities of both companies.\n",
            "    BP North America is a subsidiary of British Petroleum Co\n",
            "Plc <BP>, which also owns a 55 pct interest in Standard Oil.\n",
            "    The venture will be called BP/Standard Financial Trading\n",
            "and will be operated by Standard Oil under the oversight of a\n",
            "joint management committee.\n",
            "\n",
            " Reuter\n",
            "\n",
            "--\n",
            "docID: 3\n",
            "date: 26-FEB-1987 15:03:27.51\n",
            "title: TEXAS COMMERCE BANCSHARES <TCB> FILES PLAN\n",
            "body: Texas Commerce Bancshares Inc's Texas\n",
            "Commerce Bank-Houston said it filed an application with the\n",
            "Comptroller of the Currency in an effort to create the largest\n",
            "banking network in Harris County.\n",
            "    The bank said the network would link 31 banks having\n",
            "13.5 billion dlrs in assets and 7.5 billion dlrs in deposits.\n",
            "       \n",
            " Reuter\n",
            "\n",
            "--\n",
            "docID: 4\n",
            "date: 26-FEB-1987 15:07:13.72\n",
            "title: TALKING POINT/BANKAMERICA <BAC> EQUITY OFFER\n",
            "body: BankAmerica Corp is not under\n",
            "pressure to act quickly on its proposed equity offering and\n",
            "would do well to delay it because of the stock's recent poor\n",
            "performance, banking analysts said.\n",
            "    Some analysts said they have recommended BankAmerica delay\n",
            "its up to one-billion-dlr equity offering, which has yet to be\n",
            "approved by the Securities and Exchange Commission.\n",
            "    BankAmerica stock fell this week, along with other banking\n",
            "issues, on the news that Brazil has suspended interest payments\n",
            "on a large portion of its foreign debt.\n",
            "    The stock traded around 12, down 1/8, this afternoon,\n",
            "after falling to 11-1/2 earlier this week on the news.\n",
            "    Banking analysts said that with the immediate threat of the\n",
            "First Interstate Bancorp <I> takeover bid gone, BankAmerica is\n",
            "under no pressure to sell the securities into a market that\n",
            "will be nervous on bank stocks in the near term.\n",
            "    BankAmerica filed the offer on January 26. It was seen as\n",
            "one of the major factors leading the First Interstate\n",
            "withdrawing its takeover bid on February 9.\n",
            "    A BankAmerica spokesman said SEC approval is taking longer\n",
            "than expected and market conditions must now be re-evaluated.\n",
            "    \"The circumstances at the time will determine what we do,\"\n",
            "said Arthur Miller, BankAmerica's Vice President for Financial\n",
            "Communications, when asked if BankAmerica would proceed with\n",
            "the offer immediately after it receives SEC approval.\n",
            "    \"I'd put it off as long as they conceivably could,\" said\n",
            "Lawrence Cohn, analyst with Merrill Lynch, Pierce, Fenner and\n",
            "Smith.\n",
            "    Cohn said the longer BankAmerica waits, the longer they\n",
            "have to show the market an improved financial outlook.\n",
            "    Although BankAmerica has yet to specify the types of\n",
            "equities it would offer, most analysts believed a convertible\n",
            "preferred stock would encompass at least part of it.\n",
            "    Such an offering at a depressed stock price would mean a\n",
            "lower conversion price and more dilution to BankAmerica stock\n",
            "holders, noted Daniel Williams, analyst with Sutro Group.\n",
            "    Several analysts said that while they believe the Brazilian\n",
            "debt problem will continue to hang over the banking industry\n",
            "through the quarter, the initial shock reaction is likely to\n",
            "ease over the coming weeks.\n",
            "    Nevertheless, BankAmerica, which holds about 2.70 billion\n",
            "dlrs in Brazilian loans, stands to lose 15-20 mln dlrs if the\n",
            "interest rate is reduced on the debt, and as much as 200 mln\n",
            "dlrs if Brazil pays no interest for a year, said Joseph\n",
            "Arsenio, analyst with Birr, Wilson and Co.\n",
            "    He noted, however, that any potential losses would not show\n",
            "up in the current quarter.\n",
            "    With other major banks standing to lose even more than\n",
            "BankAmerica if Brazil fails to service its debt, the analysts\n",
            "said they expect the debt will be restructured, similar to way\n",
            "Mexico's debt was, minimizing losses to the creditor banks.\n",
            " Reuter\n",
            "\n",
            "--\n",
            "docID: 5\n",
            "date: 26-FEB-1987 15:10:44.60\n",
            "title: NATIONAL AVERAGE PRICES FOR FARMER-OWNED RESERVE\n",
            "body: The U.S. Agriculture Department\n",
            "reported the farmer-owned reserve national five-day average\n",
            "price through February 25 as follows (Dlrs/Bu-Sorghum Cwt) -\n",
            "         Natl   Loan           Release   Call\n",
            "         Avge   Rate-X  Level    Price  Price\n",
            " Wheat   2.55   2.40       IV     4.65     --\n",
            "                            V     4.65     --\n",
            "                           VI     4.45     --\n",
            " Corn    1.35   1.92       IV     3.15   3.15\n",
            "                            V     3.25     --\n",
            " X - 1986 Rates.\n",
            "\n",
            "          Natl   Loan          Release   Call\n",
            "          Avge   Rate-X  Level   Price  Price\n",
            " Oats     1.24   0.99        V    1.65    -- \n",
            " Barley   n.a.   1.56       IV    2.55   2.55\n",
            "                             V    2.65    -- \n",
            " Sorghum  2.34   3.25-Y     IV    5.36   5.36\n",
            "                             V    5.54    -- \n",
            "    Reserves I, II and III have matured. Level IV reflects\n",
            "grain entered after Oct 6, 1981 for feedgrain and after July\n",
            "23, 1981 for wheat. Level V wheat/barley after 5/14/82,\n",
            "corn/sorghum after 7/1/82. Level VI covers wheat entered after\n",
            "January 19, 1984.  X-1986 rates. Y-dlrs per CWT (100 lbs).\n",
            "n.a.-not available.\n",
            " Reuter\n",
            "\n",
            "--\n"
          ]
        }
      ],
      "source": [
        "process_collection(\"labComputerVision/reuters21578-000.xml\", print_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOYAczelb3QK"
      },
      "source": [
        "## Task 1: Complete the inverted index class\n",
        "\n",
        "  - The inverted index is an object with methods for adding and fetching postings.\n",
        "  - The data is stored in a map, where keys are terms and values are lists of postings.\n",
        "  - Each posting is an object that holds the doc_id and an optional payload."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0OIGYLqXb3QK"
      },
      "outputs": [],
      "source": [
        "# Since this is a simple data class, intializing it can be abstracted with\n",
        "# the use of dataclass decorator.\n",
        "# https://docs.python.org/3/library/dataclasses.html\n",
        "\n",
        "@dataclass\n",
        "class Posting:\n",
        "    doc_id: int\n",
        "    payload: Any = None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = defaultdict(list)\n",
        "d[\"a\"].append(1)\n",
        "d[\"a\"].append(5)\n",
        "d[\"b\"] = 2\n",
        "\n",
        "print(d[\"a\"])\n",
        "print(d[\"b\"])\n",
        "print(d[\"c\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvdaWnRKj8LN",
        "outputId": "0daf6593-ffdc-4440-984c-ef2083894027"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 5]\n",
            "2\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5Nr7SLbEb3QK"
      },
      "outputs": [],
      "source": [
        "class InvertedIndex:\n",
        "\n",
        "    def __init__(self):\n",
        "        self._index = defaultdict(list)  #term->[]->[]->[]  =>viene associata ad una word una lista\n",
        "    \n",
        "    def add_posting(self, term: str, doc_id: int, payload: Any=None) -> None:\n",
        "        \"\"\"Adds a document to the posting list of a term.\"\"\"\n",
        "        # append new posting to the posting list\n",
        "        # TODO: append new posting to the posting list\n",
        "        newValue = Posting(doc_id,payload)\n",
        "        self._index[term].append(newValue)\n",
        "\n",
        "\n",
        "    def get_postings(self, term: str) -> List[Posting]:\n",
        "        \"\"\"Fetches the posting list for a given term.\"\"\"\n",
        "        # TODO: complete\n",
        "        if self._index[term]:\n",
        "          return self._index[term]\n",
        "        return None\n",
        "\n",
        "    def get_terms(self) -> List[str]:\n",
        "        \"\"\"Returns all unique terms in the index.\"\"\"\n",
        "        return self._index.keys() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPJLvTJcb3QL"
      },
      "source": [
        "Tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LP65RIubb3QL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c28b7531-0044-4c51-869e-6bcb20aae28b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                           [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%%ipytest\n",
        "\n",
        "def test_postings():\n",
        "    ind = InvertedIndex()\n",
        "    ind.add_posting(\"term\", 1, 1)\n",
        "    ind.add_posting(\"term\", 2, 4)\n",
        "    # Testing existing term\n",
        "    postings = ind.get_postings(\"term\")\n",
        "    assert len(postings) == 2\n",
        "    assert postings[0].doc_id == 1\n",
        "    assert postings[0].payload == 1\n",
        "    assert postings[1].doc_id == 2\n",
        "    assert postings[1].payload == 4\n",
        "    # Testing non-existent term\n",
        "    assert ind.get_postings(\"xyx\") is None\n",
        "\n",
        "def test_vocabulary():\n",
        "    ind = InvertedIndex()\n",
        "    ind.add_posting(\"term1\", 1)\n",
        "    ind.add_posting(\"term2\", 1)\n",
        "    ind.add_posting(\"term3\", 2)\n",
        "    ind.add_posting(\"term2\", 3)\n",
        "    assert set(ind.get_terms()) == set([\"term1\", \"term2\", \"term3\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "MfX2u1nNb3QL"
      },
      "source": [
        "## Task 2: Build an inverted index from the input collection\n",
        "\n",
        "**TODO**: Complete the code to index the entire document collection.  (The content for each document should be the title and body concatenated)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "def get_doc_term_matrix(docs: List[str]) -> Tuple[List[int], List[str]]:\n",
        "    \"\"\"Generates a document-term matrix and the corresponding vocabulary.\n",
        "    \n",
        "    Args:\n",
        "        docs: List of documents, each given by a list of tokenized terms.\n",
        "        \n",
        "    Returns:\n",
        "        Tuple consisting of the document-term matrix and the corresponding vocabulary.\n",
        "        In the document-term matrix row `i` corresponds to `docs[i]` and column `j`\n",
        "        corresponds to the jth element of the vocabulary. Values represent the number\n",
        "        of times the term appears in the document.\n",
        "        Terms may be in any order in the vocabulary.\n",
        "    \"\"\"\n",
        "    vocabulary = [] #totale parole presenti in tutti i documenti\n",
        "    doc_term_matrix = []\n",
        "    vector = [] #una lista di dizionari che memorizzano per ogni parola del documento(key) la proprio occorrenza(value)\n",
        "    dictionary = {} #per ogni parola del documento(key) la proprio occorrenza(value)\n",
        "\n",
        "   \n",
        "    dictionary = {}\n",
        "    for word in docs:#estraggo ogni parola del documento in questione\n",
        "      if word in dictionary.keys(): #controllo se nel dizionario è presente una certa parola del doc\n",
        "        dictionary[word] = dictionary[word] +1 #se ho già la parola aumento l'occorrenza\n",
        "      else:\n",
        "        dictionary[word] =  1 #se è la prima volta che la parola si presenta nel dizionario\n",
        "      if word not in vocabulary: \n",
        "        vocabulary.append(word)\n",
        "\n",
        "\n",
        "   \n",
        "    list_vec = [] #utilizzo questa lista per popolare il nostro indice\n",
        "    for word in vocabulary:\n",
        "      if word in dictionary.keys():\n",
        "        list_vec.append(dictionary[word])\n",
        "      else:\n",
        "        list_vec.append(0)\n",
        "    return list_vec, vocabulary"
      ],
      "metadata": {
        "id": "Z7S5gqZOyp8R"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7mFqlDClb3QL"
      },
      "outputs": [],
      "source": [
        "ind = InvertedIndex()\n",
        "\n",
        "def index_doc(doc: Dict[str, Union[str, int]]) -> None:\n",
        "    \"\"\"Index document by concatenating document title and body.\n",
        "\n",
        "    Args:\n",
        "        doc: Document details.\n",
        "    \"\"\"\n",
        "    text = doc[\"title\"] + \" \" + doc[\"body\"]\n",
        "    terms = parse(text)\n",
        "    matrix, vocabolario = get_doc_term_matrix(terms)  # list of terms in the document\n",
        "    # TODO: index the document (add all terms with freqs using `ind.add_posting()`)\n",
        "    for i , count_word in enumerate(matrix):\n",
        "      ind.add_posting(vocabolario[i], i, count_word)\n",
        "\n",
        "process_collection(\"labComputerVision/reuters21578-000.xml\", index_doc)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = list(ind.get_terms())\n",
        "t = ind.get_postings(c[1])\n",
        "print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIvvweO-4CEz",
        "outputId": "590d31b8-0221-49a8-9117-1f7c0a924b80"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Posting(doc_id=1, payload=7), Posting(doc_id=2, payload=7)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxoHzwyGb3QM"
      },
      "source": [
        "## Task 3: Save the inverted index to a file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOer106Hb3QM"
      },
      "source": [
        "Save the inverted index to a file (`data/index.dat`). Use a simple text format with `termID docID1:freq1 docID2:freq2 ...` per line, e.g.,\n",
        "\n",
        "```\n",
        "xxx 1:1 2:1 3:2\n",
        "yyy 2:1 4:2\n",
        "zzz 1:3 3:1 5:2\n",
        "...\n",
        "```\n",
        "\n",
        "Implement this by (1) adding a `write_to_file(self, filename)` method to the `InvertedIndex` class and then (2) invoking that method in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2KricG55dsK",
        "outputId": "d9d38e11-e461-40eb-f7ae-05e458f25620"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labComputerVision  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "fKjR2BoKb3QM"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "all_word = list(ind.get_terms())\n",
        "file = open(\"labComputerVision/InvertedIndex.dat\"  ,\"w\")\n",
        "for word in all_word:\n",
        "  index_of_word = ind.get_postings(word)\n",
        "  data = \"\"\n",
        "  for i, word_doc in enumerate(index_of_word):\n",
        "    doc_id = word_doc.doc_id\n",
        "    payload= word_doc.payload\n",
        "    if i==0:\n",
        "      data = f\"{word} {doc_id}:{payload}\"\n",
        "    else:\n",
        "      data = data + f\" {doc_id}:{payload}\"\n",
        "  data = data + \"\\n\\n\"\n",
        "  file.write(data)\n",
        "  \n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "files.download('labComputerVision/InvertedIndex.dat')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "g_JlhXnn8K1G",
        "outputId": "f61bf75c-6679-450a-8b76-45534234ea47"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_96e9d985-0fa7-4acc-bbb1-4d85f2aa613b\", \"InvertedIndex.dat\", 456335)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UcruI0Ab3QM"
      },
      "source": [
        "## Task 4 (advanced, optional): Plot collection size against index size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVHwhhN5b3QM"
      },
      "source": [
        "Create a plot that compares the size of the document collection (bytes) against the size of the corresponding index (bytes) on the y-axis vs. with respect to the number of documents on the x-axis. You may use [Matplotlib](https://www.tutorialspoint.com/jupyter/jupyter_notebook_plotting.htm) for plotting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atpnBeO-b3QM"
      },
      "source": [
        "In our solution, we create a different callback function and use that one for indexing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXNRdf35b3QN"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "d6a0b9ba27f634b55723b9a72ccf6e1561be2239a81593bce53747f2fee7a1a2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}